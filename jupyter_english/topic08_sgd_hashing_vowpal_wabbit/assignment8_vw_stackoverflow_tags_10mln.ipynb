{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Open Machine Learning Course\n",
    "<center>\n",
    "Author: Yury Kashnitsky, Data Scientist at Mail.Ru Group\n",
    "\n",
    "This material is subject to the terms and conditions of the license [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Free use is permitted for any non-comercial purpose with an obligatory indication of the names of the authors and of the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Assignment № 8\n",
    "## <center> Vowpal Wabbit for Stackoverflow question tag classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "    1. Introduction\n",
    "    2. Data description\n",
    "    3. Data preprocessing\n",
    "    4. Training and validation of models\n",
    "    5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "In this task, you will do something that we do every week at Mail.Ru Group: train models on several GBs of data. You might cope with Python in Windows, but we strongly recommend some \\*NIX-system (for instance, with Docker) and use bash utils.\n",
    "A sad, but true, fact is that, if you want to work in the best companies in the world in ML, you will need experience with UNIX bash. Here is an interactive [tutorial](https://www.codecademy.com/en/courses/learn-the-command-line/lessons/environment/exercises/bash-profile) from CodeAcademy on UNIX command line (1-2 hours).\n",
    "\n",
    "Submit your answers through the [web-form](https://docs.google.com/forms/d/14adHGB-XKtpHlG9JJgog3DUzMUabd4y1YWG3b866m54/edit).\n",
    "\n",
    "For this particular task, you will need Vowpal Wabbit installed (we already have it inside the docker-container of our course. Check out instructions in the README in our course [repo](https://github.com/Yorko/mlcourse_open)). Make sure you have approximately 70 GB of disk space. I have tested the solution on an ordinary Macbook Pro 2015 (8 kernels, 16GB RAM), and the heaviest model was trained in ~ 12 min, so this task is doable with ordinary hardware. Still, if you have plans to rent Amazon servers, right now is a good time to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 10 GB of questions from StackOverflow – [download](https://drive.google.com/file/d/1ZU4J3KhJDrHVMj48fROFcTsTZKorPGlG/view) and unpack the archive. \n",
    "\n",
    "The data format is simple:<br>\n",
    "<center>*question text* (space dilimited words) TAB *question tags* (space delimited)\n",
    "\n",
    "TAB is the tabulation symbol.\n",
    "Let's see the first sample from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"../../raw_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is there a way to apply a background color through css at the tr level i can apply it at the td level like this my td background color e8e8e8 background e8e8e8 however the background color doesn t seem to get applied when i attempt to apply the background color at the tr level like this my tr background color e8e8e8 background e8e8e8 is there a css trick to making this work or does css not natively support this for some reason \tcss css3 css-selectors\r\n"
     ]
    }
   ],
   "source": [
    "!head -1 $PATH_TO_DATA/stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have the question text, followed by a tab and the question tags: *css, css3* and *css-selectors*. There are 10 billion of such questions in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000 ../../raw_data/stackoverflow.10kk.tsv\n",
      "CPU times: user 152 ms, sys: 17.3 ms, total: 170 ms\n",
      "Wall time: 6.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!wc -l $PATH_TO_DATA/stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we do not want to overload memory with this amount of data, so we will use the following Unix utilities - `head`, `tail`, `wc`, `cat`, `cut`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select all questions with the tags *javascript, java, python, ruby, php, c++, c#, go, scala*, and *swift* from the data source, and prepare the training set in Vowpal Wabbit's data format. We will perform 10-class question classification over the tags we've selected.\n",
    "\n",
    "In general, questions may have several tags, but we will simplify our task by selecting only one of the listed tags or dropping questions in case of no such tags.\n",
    "Note that VW supports multilabel classification (`--multilabel_oaa` parameter).\n",
    "<br>\n",
    "<br>\n",
    "Implement your data preprocessing code in a separate file `preprocess.py`. Your code must select lines with our tags and write them to a separate file in Vowpal Wabbit format. Details are as follows:\n",
    " - script must work with command line arguments: file paths for input and output\n",
    " - lines are processed one-by-one (there is a wonderful `tqdm` module for iterations counting)\n",
    " - if a line has no tab symbols or more than one tab symbol - then the line is broken, skip it\n",
    " - if a line has exactly one tab symbol, check how many tags are from our list *javascript, java, python, ruby, php, c++, c#, go, scala* or  *swift*. If there is only one tag, write the string to output with VW format: `label | text`, where `label` is a number from 1 to 10 (1 - *javascript*, ... 10 – *swift*). Skip strings with more than 1 or no tags.\n",
    " - remove `:` and `|` symbols from the question text - they have special meaning for VW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You should have 4389054 lines in the preprocessed data file. We can see that VW can process 10 GB of data in roughly 1-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello!\n",
      "100%|███████████████████████████| 10000000/10000000 [00:50<00:00, 197070.89it/s]\n"
     ]
    }
   ],
   "source": [
    "!python preprocess.py $PATH_TO_DATA/stackoverflow.10kk.tsv $PATH_TO_DATA/stackoverflow.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4389058 ../../raw_data/stackoverflow.vw\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l $PATH_TO_DATA/stackoverflow.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training, validation, and test sets in equal proportions with 1463018 lines in each file. We don't need to shuffle the data, the first 1463018 lines must go into training `stackoverflow_train.vw`, the last 1463018 lines to test `stackoverflow_test.vw`, and the rest to validation `stackoverflow_valid.vw`. \n",
    "\n",
    "Save answer vectors for validation and test sets into separate files: `stackoverflow_valid_labels.txt` and `stackoverflow_test_labels.txt`, respectively.\n",
    "\n",
    "Do not hesitate to use `head`, `tail`, `split`, `cat` and `cut` linux utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "!split -l 1463018 $PATH_TO_DATA/stackoverflow.vw stackoverflow_\n",
    "!mv stackoverflow_aa stackoverflow_train.vw\n",
    "!mv stackoverflow_ab stackoverflow_test.vw\n",
    "!mv stackoverflow_ac stackoverflow_valid.vw\n",
    "!cut -d'|' -f1 stackoverflow_valid.vw > stackoverflow_valid_labels.txt\n",
    "!cut -d'|' -f1 stackoverflow_test.vw > stackoverflow_test_labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training and validation of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Vowpal Wabbit with `stackoverflow_train.vw` 9 times with (1,3,5) iterating passes and n-gram (n=1,2,3) parameters.\n",
    "The rest of the parameters are `bit_precision=28` and `seed=17`. Don't forget to tell VW that we have a 10-class problem.\n",
    "\n",
    "Evaluate accuracy on `stackoverflow_valid.vw`. Choose the model with the best parameters, and test it on `stackoverflow_test.vw` set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred\n",
       "0     1\n",
       "1     5\n",
       "2     7\n",
       "3     2\n",
       "4     9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = pd.read_csv('stackoverflow_valid_labels.txt', names=['pred'])\n",
    "y_true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vw -d stackoverflow_train.vw --oaa 10 --ngram 1 --passes 1 -b 28 --random_seed 17 --readable_model 1_1readable.vw.model -f 1_1vw.model -c\n",
      "Generating 1-grams for all namespaces.\n",
      "final_regressor = 1_1vw.model\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using cache_file = stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      161\n",
      "0.500000 1.000000            2            2.0        4        1       68\n",
      "0.750000 1.000000            4            4.0        7        1       88\n",
      "0.750000 0.750000            8            8.0        7        1       95\n",
      "0.812500 0.875000           16           16.0        7        7      209\n",
      "0.812500 0.812500           32           32.0        7        5      174\n",
      "0.781250 0.750000           64           64.0        3        3      204\n",
      "0.671875 0.562500          128          128.0        1        5       29\n",
      "0.609375 0.546875          256          256.0        5        5      169\n",
      "0.541016 0.472656          512          512.0        2        2      303\n",
      "0.451172 0.361328         1024         1024.0        3        3      123\n",
      "0.375977 0.300781         2048         2048.0        1        5       83\n",
      "0.311523 0.247070         4096         4096.0        1        1       79\n",
      "0.269287 0.227051         8192         8192.0        2        2      112\n",
      "0.226868 0.184448        16384        16384.0        7        7      252\n",
      "0.188995 0.151123        32768        32768.0        4        5      134\n",
      "0.162964 0.136932        65536        65536.0        5        5      145\n",
      "0.141556 0.120148       131072       131072.0        7        2      255\n",
      "0.123825 0.106094       262144       262144.0        7        7      101\n",
      "0.112268 0.100712       524288       524288.0        1        1      818\n",
      "0.103132 0.093996      1048576      1048576.0        1        1      571\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.098885\n",
      "total feature number = 291954690\n",
      "vw -d stackoverflow_valid.vw -t -i 1_1vw.model -p stackoverflow_valid_preds.txt\n",
      "Generating 1-grams for all namespaces.\n",
      "only testing\n",
      "predictions = stackoverflow_valid_preds.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      122\n",
      "0.000000 0.000000            2            2.0        5        5      187\n",
      "0.000000 0.000000            4            4.0        2        2      136\n",
      "0.000000 0.000000            8            8.0        5        5       80\n",
      "0.000000 0.000000           16           16.0        6        6      174\n",
      "0.031250 0.062500           32           32.0        2        2      649\n",
      "0.062500 0.093750           64           64.0        1        5       57\n",
      "0.085938 0.109375          128          128.0        1        1       92\n",
      "0.066406 0.046875          256          256.0        2        2       42\n",
      "0.068359 0.070312          512          512.0        5        5      152\n",
      "0.080078 0.091797         1024         1024.0        2        2      103\n",
      "0.081543 0.083008         2048         2048.0        6        6      148\n",
      "0.081055 0.080566         4096         4096.0        1        1      126\n",
      "0.084106 0.087158         8192         8192.0        1        1      201\n",
      "0.084656 0.085205        16384        16384.0        2        2      132\n",
      "0.084808 0.084961        32768        32768.0        6        6      170\n",
      "0.085388 0.085968        65536        65536.0        1        1     3000\n",
      "0.084991 0.084595       131072       131072.0        7        7      111\n",
      "0.085232 0.085472       262144       262144.0        7        7      114\n",
      "0.085135 0.085037       524288       524288.0        7        7      375\n",
      "0.084913 0.084692      1048576      1048576.0        7        7       25\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.084911\n",
      "total feature number = 292619465\n",
      "With 1 ngrams and 1 passes got 0.915089\n",
      "vw -d stackoverflow_train.vw --oaa 10 --ngram 1 --passes 3 -b 28 --random_seed 17 --readable_model 1_3readable.vw.model -f 1_3vw.model -c\n",
      "Generating 1-grams for all namespaces.\n",
      "final_regressor = 1_3vw.model\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      161\n",
      "0.500000 1.000000            2            2.0        4        1       68\n",
      "0.750000 1.000000            4            4.0        7        1       88\n",
      "0.750000 0.750000            8            8.0        7        1       95\n",
      "0.750000 0.750000           16           16.0        2        7      159\n",
      "0.781250 0.812500           32           32.0        1        7      404\n",
      "0.718750 0.656250           64           64.0        7        7      103\n",
      "0.664062 0.609375          128          128.0        5        5      276\n",
      "0.625000 0.585938          256          256.0        1        1      102\n",
      "0.546875 0.468750          512          512.0        2        5       68\n",
      "0.450195 0.353516         1024         1024.0        1        1      132\n",
      "0.387207 0.324219         2048         2048.0        7        7       71\n",
      "0.318115 0.249023         4096         4096.0        2        2      319\n",
      "0.267578 0.217041         8192         8192.0        5        2       24\n",
      "0.227661 0.187744        16384        16384.0        3        3      581\n",
      "0.190674 0.153687        32768        32768.0        3        3       28\n",
      "0.164215 0.137756        65536        65536.0        4        4      184\n",
      "0.142067 0.119919       131072       131072.0        2        2       95\n",
      "0.125511 0.108955       262144       262144.0        5        5      232\n",
      "0.112450 0.099388       524288       524288.0        6        6      142\n",
      "0.103465 0.094481      1048576      1048576.0        1        1      422\n",
      "0.096127 0.096127      2097152      2097152.0        5        5      696 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 3\n",
      "weighted example sum = 3950151.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.087949 h\n",
      "total feature number = 788274150\n",
      "vw -d stackoverflow_valid.vw -t -i 1_3vw.model -p stackoverflow_valid_preds.txt\n",
      "Generating 1-grams for all namespaces.\n",
      "only testing\n",
      "predictions = stackoverflow_valid_preds.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      122\n",
      "0.000000 0.000000            2            2.0        5        5      187\n",
      "0.000000 0.000000            4            4.0        2        2      136\n",
      "0.125000 0.250000            8            8.0        5        5       80\n",
      "0.062500 0.000000           16           16.0        6        6      174\n",
      "0.062500 0.062500           32           32.0        2        2      649\n",
      "0.078125 0.093750           64           64.0        1        5       57\n",
      "0.085938 0.093750          128          128.0        1        1       92\n",
      "0.066406 0.046875          256          256.0        2        2       42\n",
      "0.062500 0.058594          512          512.0        5        5      152\n",
      "0.071289 0.080078         1024         1024.0        2        2      103\n",
      "0.079102 0.086914         2048         2048.0        6        6      148\n",
      "0.079102 0.079102         4096         4096.0        1        1      126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.082886 0.086670         8192         8192.0        1        1      201\n",
      "0.084412 0.085938        16384        16384.0        2        2      132\n",
      "0.085602 0.086792        32768        32768.0        6        6      170\n",
      "0.086472 0.087341        65536        65536.0        1        1     3000\n",
      "0.085915 0.085358       131072       131072.0        7        7      111\n",
      "0.085800 0.085686       262144       262144.0        7        7      114\n",
      "0.085659 0.085518       524288       524288.0        7        7      375\n",
      "0.085511 0.085363      1048576      1048576.0        7        7       25\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.085552\n",
      "total feature number = 292619465\n",
      "With 1 ngrams and 3 passes got 0.914448\n",
      "vw -d stackoverflow_train.vw --oaa 10 --ngram 1 --passes 5 -b 28 --random_seed 17 --readable_model 1_5readable.vw.model -f 1_5vw.model -c\n",
      "Generating 1-grams for all namespaces.\n",
      "final_regressor = 1_5vw.model\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      161\n",
      "0.500000 1.000000            2            2.0        4        1       68\n",
      "0.750000 1.000000            4            4.0        7        1       88\n",
      "0.750000 0.750000            8            8.0        7        1       95\n",
      "0.750000 0.750000           16           16.0        2        7      159\n",
      "0.781250 0.812500           32           32.0        1        7      404\n",
      "0.718750 0.656250           64           64.0        7        7      103\n",
      "0.664062 0.609375          128          128.0        5        5      276\n",
      "0.625000 0.585938          256          256.0        1        1      102\n",
      "0.546875 0.468750          512          512.0        2        5       68\n",
      "0.450195 0.353516         1024         1024.0        1        1      132\n",
      "0.387207 0.324219         2048         2048.0        7        7       71\n",
      "0.318115 0.249023         4096         4096.0        2        2      319\n",
      "0.267578 0.217041         8192         8192.0        5        2       24\n",
      "0.227661 0.187744        16384        16384.0        3        3      581\n",
      "0.190674 0.153687        32768        32768.0        3        3       28\n",
      "0.164215 0.137756        65536        65536.0        4        4      184\n",
      "0.142067 0.119919       131072       131072.0        2        2       95\n",
      "0.125511 0.108955       262144       262144.0        5        5      232\n",
      "0.112450 0.099388       524288       524288.0        6        6      142\n",
      "0.103465 0.094481      1048576      1048576.0        1        1      422\n",
      "0.096127 0.096127      2097152      2097152.0        5        5      696 h\n",
      "0.092037 0.087947      4194304      4194304.0        1        1      216 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 5\n",
      "weighted example sum = 6583585.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.087949 h\n",
      "total feature number = 1313790250\n",
      "vw -d stackoverflow_valid.vw -t -i 1_5vw.model -p stackoverflow_valid_preds.txt\n",
      "Generating 1-grams for all namespaces.\n",
      "only testing\n",
      "predictions = stackoverflow_valid_preds.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        1        5      122\n",
      "0.500000 0.000000            2            2.0        5        5      187\n",
      "0.250000 0.000000            4            4.0        2        2      136\n",
      "0.250000 0.250000            8            8.0        5        5       80\n",
      "0.125000 0.000000           16           16.0        6        6      174\n",
      "0.125000 0.125000           32           32.0        2        2      649\n",
      "0.109375 0.093750           64           64.0        1        5       57\n",
      "0.109375 0.109375          128          128.0        1        1       92\n",
      "0.078125 0.046875          256          256.0        2        2       42\n",
      "0.068359 0.058594          512          512.0        5        5      152\n",
      "0.076172 0.083984         1024         1024.0        2        2      103\n",
      "0.082031 0.087891         2048         2048.0        6        6      148\n",
      "0.082031 0.082031         4096         4096.0        1        1      126\n",
      "0.083374 0.084717         8192         8192.0        1        1      201\n",
      "0.084717 0.086060        16384        16384.0        2        2      132\n",
      "0.086517 0.088318        32768        32768.0        6        6      170\n",
      "0.086746 0.086975        65536        65536.0        1        1     3000\n",
      "0.086029 0.085312       131072       131072.0        7        7      111\n",
      "0.085983 0.085938       262144       262144.0        7        7      114\n",
      "0.085970 0.085957       524288       524288.0        7        7      375\n",
      "0.086017 0.086063      1048576      1048576.0        7        7       25\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.086031\n",
      "total feature number = 292619465\n",
      "With 1 ngrams and 5 passes got 0.913969\n",
      "vw -d stackoverflow_train.vw --oaa 10 --ngram 2 --passes 1 -b 28 --random_seed 17 --readable_model 2_1readable.vw.model -f 2_1vw.model -c\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = 2_1vw.model\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using cache_file = stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.750000 0.750000           16           16.0        7        7      416\n",
      "0.781250 0.812500           32           32.0        7        2      346\n",
      "0.765625 0.750000           64           64.0        3        3      406\n",
      "0.664062 0.562500          128          128.0        1        7       56\n",
      "0.597656 0.531250          256          256.0        5        3      336\n",
      "0.527344 0.457031          512          512.0        2        2      604\n",
      "0.429688 0.332031         1024         1024.0        3        3      244\n",
      "0.362793 0.295898         2048         2048.0        1        5      164\n",
      "0.298096 0.233398         4096         4096.0        1        1      156\n",
      "0.249878 0.201660         8192         8192.0        2        2      222\n",
      "0.210266 0.170654        16384        16384.0        7        7      502\n",
      "0.175262 0.140259        32768        32768.0        4        5      266\n",
      "0.148163 0.121063        65536        65536.0        5        5      288\n",
      "0.127380 0.106598       131072       131072.0        7        2      508\n",
      "0.109459 0.091537       262144       262144.0        7        7      200\n",
      "0.097469 0.085480       524288       524288.0        1        1     1634\n",
      "0.087223 0.076977      1048576      1048576.0        1        1     1140\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.083143\n",
      "total feature number = 580983344\n",
      "vw -d stackoverflow_valid.vw -t -i 2_1vw.model -p stackoverflow_valid_preds.txt\n",
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = stackoverflow_valid_preds.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      242\n",
      "0.000000 0.000000            2            2.0        5        5      372\n",
      "0.250000 0.500000            4            4.0        2        2      270\n",
      "0.125000 0.000000            8            8.0        5        5      158\n",
      "0.062500 0.000000           16           16.0        6        6      346\n",
      "0.062500 0.062500           32           32.0        2        2     1296\n",
      "0.062500 0.062500           64           64.0        1        5      112\n",
      "0.062500 0.062500          128          128.0        1        1      182\n",
      "0.066406 0.070312          256          256.0        2        1       82\n",
      "0.072266 0.078125          512          512.0        5        5      302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.065430 0.058594         1024         1024.0        2        2      204\n",
      "0.070801 0.076172         2048         2048.0        6        6      294\n",
      "0.071045 0.071289         4096         4096.0        1        1      250\n",
      "0.071655 0.072266         8192         8192.0        1        1      400\n",
      "0.070618 0.069580        16384        16384.0        2        2      262\n",
      "0.070099 0.069580        32768        32768.0        6        6      338\n",
      "0.070480 0.070862        65536        65536.0        1        1     5998\n",
      "0.069366 0.068253       131072       131072.0        7        7      220\n",
      "0.068985 0.068604       262144       262144.0        7        7      226\n",
      "0.069120 0.069256       524288       524288.0        7        7      748\n",
      "0.069041 0.068962      1048576      1048576.0        7        2       48\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.068809\n",
      "total feature number = 582312894\n",
      "With 2 ngrams and 1 passes got 0.931191\n",
      "vw -d stackoverflow_train.vw --oaa 10 --ngram 2 --passes 3 -b 28 --random_seed 17 --readable_model 2_3readable.vw.model -f 2_3vw.model -c\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = 2_3vw.model\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.687500 0.625000           16           16.0        2        7      316\n",
      "0.687500 0.687500           32           32.0        1        7      806\n",
      "0.687500 0.687500           64           64.0        7        7      204\n",
      "0.601562 0.515625          128          128.0        5        5      550\n",
      "0.574219 0.546875          256          256.0        1        1      202\n",
      "0.503906 0.433594          512          512.0        2        5      134\n",
      "0.432617 0.361328         1024         1024.0        1        1      262\n",
      "0.370605 0.308594         2048         2048.0        7        7      140\n",
      "0.299072 0.227539         4096         4096.0        2        2      636\n",
      "0.252197 0.205322         8192         8192.0        5        2       46\n",
      "0.215515 0.178833        16384        16384.0        3        3     1160\n",
      "0.178619 0.141724        32768        32768.0        3        3       54\n",
      "0.149918 0.121216        65536        65536.0        4        4      366\n",
      "0.128204 0.106491       131072       131072.0        2        2      188\n",
      "0.111256 0.094307       262144       262144.0        5        5      462\n",
      "0.097771 0.084286       524288       524288.0        6        6      282\n",
      "0.087827 0.077883      1048576      1048576.0        1        1      842\n",
      "0.079544 0.079544      2097152      2097152.0        5        5     1390 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 3\n",
      "weighted example sum = 3950151.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.071968 h\n",
      "total feature number = 1568647998\n",
      "vw -d stackoverflow_valid.vw -t -i 2_3vw.model -p stackoverflow_valid_preds.txt\n",
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = stackoverflow_valid_preds.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      242\n",
      "0.000000 0.000000            2            2.0        5        5      372\n",
      "0.250000 0.500000            4            4.0        2        2      270\n",
      "0.125000 0.000000            8            8.0        5        5      158\n",
      "0.062500 0.000000           16           16.0        6        6      346\n",
      "0.062500 0.062500           32           32.0        2        2     1296\n",
      "0.062500 0.062500           64           64.0        1        5      112\n",
      "0.062500 0.062500          128          128.0        1        1      182\n",
      "0.062500 0.062500          256          256.0        2        1       82\n",
      "0.064453 0.066406          512          512.0        5        5      302\n",
      "0.066406 0.068359         1024         1024.0        2        2      204\n",
      "0.071777 0.077148         2048         2048.0        6        6      294\n",
      "0.071533 0.071289         4096         4096.0        1        1      250\n",
      "0.073608 0.075684         8192         8192.0        1        1      400\n",
      "0.071350 0.069092        16384        16384.0        2        2      262\n",
      "0.072693 0.074036        32768        32768.0        6        6      338\n",
      "0.073517 0.074341        65536        65536.0        1        1     5998\n",
      "0.072342 0.071167       131072       131072.0        7        7      220\n",
      "0.072269 0.072197       262144       262144.0        7        7      226\n",
      "0.072493 0.072716       524288       524288.0        7        7      748\n",
      "0.072300 0.072107      1048576      1048576.0        7        7       48\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.072086\n",
      "total feature number = 582312894\n",
      "With 2 ngrams and 3 passes got 0.927914\n",
      "vw -d stackoverflow_train.vw --oaa 10 --ngram 2 --passes 5 -b 28 --random_seed 17 --readable_model 2_5readable.vw.model -f 2_5vw.model -c\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = 2_5vw.model\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.687500 0.625000           16           16.0        2        7      316\n",
      "0.687500 0.687500           32           32.0        1        7      806\n",
      "0.687500 0.687500           64           64.0        7        7      204\n",
      "0.601562 0.515625          128          128.0        5        5      550\n",
      "0.574219 0.546875          256          256.0        1        1      202\n",
      "0.503906 0.433594          512          512.0        2        5      134\n",
      "0.432617 0.361328         1024         1024.0        1        1      262\n",
      "0.370605 0.308594         2048         2048.0        7        7      140\n",
      "0.299072 0.227539         4096         4096.0        2        2      636\n",
      "0.252197 0.205322         8192         8192.0        5        2       46\n",
      "0.215515 0.178833        16384        16384.0        3        3     1160\n",
      "0.178619 0.141724        32768        32768.0        3        3       54\n",
      "0.149918 0.121216        65536        65536.0        4        4      366\n",
      "0.128204 0.106491       131072       131072.0        2        2      188\n",
      "0.111256 0.094307       262144       262144.0        5        5      462\n",
      "0.097771 0.084286       524288       524288.0        6        6      282\n",
      "0.087827 0.077883      1048576      1048576.0        1        1      842\n",
      "0.079544 0.079544      2097152      2097152.0        5        5     1390 h\n",
      "0.075875 0.072205      4194304      4194304.0        1        1      430 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 5\n",
      "weighted example sum = 6583585.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.071968 h\n",
      "total feature number = 2614413330\n",
      "vw -d stackoverflow_valid.vw -t -i 2_5vw.model -p stackoverflow_valid_preds.txt\n",
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = stackoverflow_valid_preds.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using no cache\n",
      "Reading datafile = stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      242\n",
      "0.000000 0.000000            2            2.0        5        5      372\n",
      "0.250000 0.500000            4            4.0        2        2      270\n",
      "0.125000 0.000000            8            8.0        5        5      158\n",
      "0.062500 0.000000           16           16.0        6        6      346\n",
      "0.062500 0.062500           32           32.0        2        2     1296\n",
      "0.062500 0.062500           64           64.0        1        5      112\n",
      "0.062500 0.062500          128          128.0        1        1      182\n",
      "0.062500 0.062500          256          256.0        2        1       82\n",
      "0.070312 0.078125          512          512.0        5        5      302\n",
      "0.067383 0.064453         1024         1024.0        2        2      204\n",
      "0.070801 0.074219         2048         2048.0        6        6      294\n",
      "0.070312 0.069824         4096         4096.0        1        1      250\n",
      "0.070923 0.071533         8192         8192.0        1        1      400\n",
      "0.069702 0.068481        16384        16384.0        2        2      262\n",
      "0.070740 0.071777        32768        32768.0        6        6      338\n",
      "0.071609 0.072479        65536        65536.0        1        1     5998\n",
      "0.070656 0.069702       131072       131072.0        7        7      220\n",
      "0.070786 0.070915       262144       262144.0        7        7      226\n",
      "0.071072 0.071358       524288       524288.0        7        7      748\n",
      "0.070870 0.070669      1048576      1048576.0        7        7       48\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.070616\n",
      "total feature number = 582312894\n",
      "With 2 ngrams and 5 passes got 0.929384\n",
      "vw -d stackoverflow_train.vw --oaa 10 --ngram 3 --passes 1 -b 28 --random_seed 17 --readable_model 3_1readable.vw.model -f 3_1vw.model -c\n",
      "Generating 3-grams for all namespaces.\n",
      "final_regressor = 3_1vw.model\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using cache_file = stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      478\n",
      "0.500000 1.000000            2            2.0        4        1      199\n",
      "0.750000 1.000000            4            4.0        7        1      259\n",
      "0.750000 0.750000            8            8.0        7        1      280\n",
      "0.750000 0.750000           16           16.0        7        7      622\n",
      "0.781250 0.812500           32           32.0        7        2      517\n",
      "0.765625 0.750000           64           64.0        3        3      607\n",
      "0.656250 0.546875          128          128.0        1        7       82\n",
      "0.605469 0.554688          256          256.0        5        1      502\n",
      "0.535156 0.464844          512          512.0        2        2      904\n",
      "0.442383 0.349609         1024         1024.0        3        3      364\n",
      "0.380371 0.318359         2048         2048.0        1        5      244\n",
      "0.317383 0.254395         4096         4096.0        1        1      232\n",
      "0.266724 0.216064         8192         8192.0        2        2      331\n",
      "0.222290 0.177856        16384        16384.0        7        7      751\n",
      "0.184631 0.146973        32768        32768.0        4        5      397\n",
      "0.155502 0.126373        65536        65536.0        5        5      430\n",
      "0.132919 0.110336       131072       131072.0        7        7      760\n",
      "0.113468 0.094017       262144       262144.0        7        7      298\n",
      "0.101650 0.089832       524288       524288.0        1        1     2449\n",
      "0.090106 0.078562      1048576      1048576.0        1        1     1708\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.086319\n",
      "total feature number = 868548985\n",
      "vw -d stackoverflow_valid.vw -t -i 3_1vw.model -p stackoverflow_valid_preds.txt\n",
      "Generating 3-grams for all namespaces.\n",
      "only testing\n",
      "predictions = stackoverflow_valid_preds.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      361\n",
      "0.000000 0.000000            2            2.0        5        5      556\n",
      "0.000000 0.000000            4            4.0        2        2      403\n",
      "0.000000 0.000000            8            8.0        5        5      235\n",
      "0.062500 0.125000           16           16.0        6        6      517\n",
      "0.062500 0.062500           32           32.0        2        2     1942\n",
      "0.062500 0.062500           64           64.0        1        5      166\n",
      "0.070312 0.078125          128          128.0        1        1      271\n",
      "0.078125 0.085938          256          256.0        2        1      121\n",
      "0.076172 0.074219          512          512.0        5        5      451\n",
      "0.073242 0.070312         1024         1024.0        2        2      304\n",
      "0.078613 0.083984         2048         2048.0        6        6      439\n",
      "0.076904 0.075195         4096         4096.0        1        1      373\n",
      "0.073486 0.070068         8192         8192.0        1        1      598\n",
      "0.072388 0.071289        16384        16384.0        2        2      391\n",
      "0.072479 0.072571        32768        32768.0        6        6      505\n",
      "0.073044 0.073608        65536        65536.0        1        1     8995\n",
      "0.071892 0.070740       131072       131072.0        7        7      328\n",
      "0.071377 0.070862       262144       262144.0        7        7      337\n",
      "0.071609 0.071842       524288       524288.0        7        7     1120\n",
      "0.071566 0.071522      1048576      1048576.0        7        2       70\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.071320\n",
      "total feature number = 870543306\n",
      "With 3 ngrams and 1 passes got 0.928680\n",
      "vw -d stackoverflow_train.vw --oaa 10 --ngram 3 --passes 3 -b 28 --random_seed 17 --readable_model 3_3readable.vw.model -f 3_3vw.model -c\n",
      "Generating 3-grams for all namespaces.\n",
      "final_regressor = 3_3vw.model\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      478\n",
      "0.500000 1.000000            2            2.0        4        1      199\n",
      "0.750000 1.000000            4            4.0        7        1      259\n",
      "0.750000 0.750000            8            8.0        7        1      280\n",
      "0.750000 0.750000           16           16.0        2        7      472\n",
      "0.750000 0.750000           32           32.0        1        7     1207\n",
      "0.718750 0.687500           64           64.0        7        7      304\n",
      "0.640625 0.562500          128          128.0        5        5      823\n",
      "0.609375 0.578125          256          256.0        1        1      301\n",
      "0.521484 0.433594          512          512.0        2        5      199\n",
      "0.442383 0.363281         1024         1024.0        1        1      391\n",
      "0.378906 0.315430         2048         2048.0        7        7      208\n",
      "0.310791 0.242676         4096         4096.0        2        2      952\n",
      "0.265747 0.220703         8192         8192.0        5        2       67\n",
      "0.223755 0.181763        16384        16384.0        3        3     1738\n",
      "0.186035 0.148315        32768        32768.0        3        3       79\n",
      "0.155670 0.125305        65536        65536.0        4        4      547\n",
      "0.133133 0.110596       131072       131072.0        2        2      280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.115284 0.097435       262144       262144.0        5        5      691\n",
      "0.101097 0.086910       524288       524288.0        6        6      421\n",
      "0.090279 0.079460      1048576      1048576.0        1        1     1261\n",
      "0.081248 0.081248      2097152      2097152.0        5        5     2083 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 3\n",
      "weighted example sum = 3950151.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.074053 h\n",
      "total feature number = 2345071710\n",
      "vw -d stackoverflow_valid.vw -t -i 3_3vw.model -p stackoverflow_valid_preds.txt\n",
      "Generating 3-grams for all namespaces.\n",
      "only testing\n",
      "predictions = stackoverflow_valid_preds.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      361\n",
      "0.000000 0.000000            2            2.0        5        5      556\n",
      "0.000000 0.000000            4            4.0        2        2      403\n",
      "0.000000 0.000000            8            8.0        5        5      235\n",
      "0.000000 0.000000           16           16.0        6        6      517\n",
      "0.031250 0.062500           32           32.0        2        2     1942\n",
      "0.046875 0.062500           64           64.0        1        5      166\n",
      "0.054688 0.062500          128          128.0        1        1      271\n",
      "0.070312 0.085938          256          256.0        2        1      121\n",
      "0.074219 0.078125          512          512.0        5        5      451\n",
      "0.073242 0.072266         1024         1024.0        2        2      304\n",
      "0.078125 0.083008         2048         2048.0        6        6      439\n",
      "0.076904 0.075684         4096         4096.0        1        1      373\n",
      "0.075439 0.073975         8192         8192.0        1        1      598\n",
      "0.074707 0.073975        16384        16384.0        2        2      391\n",
      "0.075226 0.075745        32768        32768.0        6        6      505\n",
      "0.075500 0.075775        65536        65536.0        1        1     8995\n",
      "0.074425 0.073349       131072       131072.0        7        7      328\n",
      "0.074295 0.074165       262144       262144.0        7        7      337\n",
      "0.074203 0.074112       524288       524288.0        7        7     1120\n",
      "0.074096 0.073988      1048576      1048576.0        7        2       70\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.073795\n",
      "total feature number = 870543306\n",
      "With 3 ngrams and 3 passes got 0.926205\n",
      "vw -d stackoverflow_train.vw --oaa 10 --ngram 3 --passes 5 -b 28 --random_seed 17 --readable_model 3_5readable.vw.model -f 3_5vw.model -c\n",
      "Generating 3-grams for all namespaces.\n",
      "final_regressor = 3_5vw.model\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      478\n",
      "0.500000 1.000000            2            2.0        4        1      199\n",
      "0.750000 1.000000            4            4.0        7        1      259\n",
      "0.750000 0.750000            8            8.0        7        1      280\n",
      "0.750000 0.750000           16           16.0        2        7      472\n",
      "0.750000 0.750000           32           32.0        1        7     1207\n",
      "0.718750 0.687500           64           64.0        7        7      304\n",
      "0.640625 0.562500          128          128.0        5        5      823\n",
      "0.609375 0.578125          256          256.0        1        1      301\n",
      "0.521484 0.433594          512          512.0        2        5      199\n",
      "0.442383 0.363281         1024         1024.0        1        1      391\n",
      "0.378906 0.315430         2048         2048.0        7        7      208\n",
      "0.310791 0.242676         4096         4096.0        2        2      952\n",
      "0.265747 0.220703         8192         8192.0        5        2       67\n",
      "0.223755 0.181763        16384        16384.0        3        3     1738\n",
      "0.186035 0.148315        32768        32768.0        3        3       79\n",
      "0.155670 0.125305        65536        65536.0        4        4      547\n",
      "0.133133 0.110596       131072       131072.0        2        2      280\n",
      "0.115284 0.097435       262144       262144.0        5        5      691\n",
      "0.101097 0.086910       524288       524288.0        6        6      421\n",
      "0.090279 0.079460      1048576      1048576.0        1        1     1261\n",
      "0.081248 0.081248      2097152      2097152.0        5        5     2083 h\n",
      "0.077922 0.074596      4194304      4194304.0        1        1      643 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 5\n",
      "weighted example sum = 6583585.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.074053 h\n",
      "total feature number = 3908452850\n",
      "vw -d stackoverflow_valid.vw -t -i 3_5vw.model -p stackoverflow_valid_preds.txt\n",
      "Generating 3-grams for all namespaces.\n",
      "only testing\n",
      "predictions = stackoverflow_valid_preds.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      361\n",
      "0.000000 0.000000            2            2.0        5        5      556\n",
      "0.000000 0.000000            4            4.0        2        2      403\n",
      "0.000000 0.000000            8            8.0        5        5      235\n",
      "0.062500 0.125000           16           16.0        6        6      517\n",
      "0.062500 0.062500           32           32.0        2        2     1942\n",
      "0.062500 0.062500           64           64.0        1        5      166\n",
      "0.062500 0.062500          128          128.0        1        1      271\n",
      "0.074219 0.085938          256          256.0        2        1      121\n",
      "0.072266 0.070312          512          512.0        5        5      451\n",
      "0.073242 0.074219         1024         1024.0        2        2      304\n",
      "0.078125 0.083008         2048         2048.0        6        6      439\n",
      "0.078369 0.078613         4096         4096.0        1        1      373\n",
      "0.075684 0.072998         8192         8192.0        1        1      598\n",
      "0.074341 0.072998        16384        16384.0        2        2      391\n",
      "0.074707 0.075073        32768        32768.0        6        6      505\n",
      "0.075439 0.076172        65536        65536.0        1        1     8995\n",
      "0.074394 0.073349       131072       131072.0        7        7      328\n",
      "0.074135 0.073875       262144       262144.0        7        7      337\n",
      "0.074186 0.074238       524288       524288.0        7        7     1120\n",
      "0.074080 0.073973      1048576      1048576.0        7        2       70\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.073841\n",
      "total feature number = 870543306\n",
      "With 3 ngrams and 5 passes got 0.926159\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "#from subprocess import check_call\n",
    "\n",
    "i = 1\n",
    "\n",
    "accuracies = []\n",
    "settings = []\n",
    "for ngram in [1, 2, 3]:\n",
    "    for passes in [1,3,5]:\n",
    "        train_cmd = \"vw -d stackoverflow_train.vw --oaa 10 --ngram {ng} --passes {ps} \\\n",
    "        -b 28 --random_seed 17 --readable_model {ng}_{ps}readable.vw.model -f {ng}_{ps}vw.model -c\".format(ng=ngram, ps=passes)\n",
    "        ! echo $train_cmd\n",
    "        ! $train_cmd\n",
    "        test_cmd = \"vw -d stackoverflow_valid.vw -t -i {ng}_{ps}vw.model -p stackoverflow_valid_preds.txt\".format(ng=ngram, ps=passes)\n",
    "        ! echo $test_cmd\n",
    "        ! $test_cmd\n",
    "        y_pred = pd.read_csv('stackoverflow_valid_preds.txt', names=['pred'])\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        accuracies.append(acc)\n",
    "        settings.append(\"n%d p%d\" % (ngram, passes))\n",
    "        print(\"With %d ngrams and %d passes got %f\" % (ngram, passes, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('n1 p1', 0.9150885361629181), ('n1 p3', 0.9144480792444112), ('n1 p5', 0.913968932713063), ('n2 p1', 0.9311908670980125), ('n2 p3', 0.927914078979206), ('n2 p5', 0.9293836439469644), ('n3 p1', 0.9286803033182094), ('n3 p3', 0.9262045989864787), ('n3 p5', 0.9261594867595614)]\n",
      "n2 p1\n",
      "0.9311908670980125\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(settings,accuracies)))\n",
    "print(settings[np.argmax(accuracies)])\n",
    "print(max(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Which parameter set provides the best accuracy on the validation set `stackoverflow_valid.vw`?\n",
    "- bigrams and 3 passes\n",
    "- trigrams and 5 passes\n",
    "- ##### bigrams and 1 pass\n",
    "- unigrams and 1 pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the best (according to validation accuracy) model on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vw -d stackoverflow_test.vw -t -i 2_1vw.model -p stackoverflow_test_preds.txt\n",
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = stackoverflow_test_preds.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = stackoverflow_test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        7      354\n",
      "0.500000 0.000000            2            2.0        7        7      146\n",
      "0.250000 0.000000            4            4.0        5        5      516\n",
      "0.125000 0.000000            8            8.0        7        7      286\n",
      "0.125000 0.125000           16           16.0        6        6      716\n",
      "0.062500 0.000000           32           32.0        2        2      798\n",
      "0.062500 0.062500           64           64.0        5        5     2120\n",
      "0.046875 0.031250          128          128.0        2        2      262\n",
      "0.074219 0.101562          256          256.0        6        6      174\n",
      "0.058594 0.042969          512          512.0        7        7       68\n",
      "0.057617 0.056641         1024         1024.0        1        1      174\n",
      "0.062500 0.067383         2048         2048.0        2        2      484\n",
      "0.063965 0.065430         4096         4096.0        2        2      524\n",
      "0.066528 0.069092         8192         8192.0        7        7      542\n",
      "0.069092 0.071655        16384        16384.0        2        2      252\n",
      "0.068420 0.067749        32768        32768.0        1        1      240\n",
      "0.069077 0.069733        65536        65536.0        2        2      148\n",
      "0.068680 0.068283       131072       131072.0        2        2       30\n",
      "0.068432 0.068184       262144       262144.0        6        6      162\n",
      "0.068745 0.069057       524288       524288.0        5        5      644\n",
      "0.068954 0.069164      1048576      1048576.0        4        2      210\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.068918\n",
      "total feature number = 580609416\n",
      "0.9310815041236676\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "y_true_test = pd.read_csv('stackoverflow_test_labels.txt', names=['pred'])\n",
    "test_cmd = \"vw -d stackoverflow_test.vw -t -i 2_1vw.model -p stackoverflow_test_preds.txt\"\n",
    "! echo $test_cmd\n",
    "! $test_cmd\n",
    "y_pred = pd.read_csv('stackoverflow_test_preds.txt', names=['pred'])\n",
    "acc = accuracy_score(y_true_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Compare best validation and test accuracies. Choose the correct answer (% is a percent here i.e. a drop from 50% to 40% would be 10%, not 20%).\n",
    "- Test accuracy is lower by approx. 2%\n",
    "- Test accuracy is lower by approx. 3%\n",
    "- ##### difference is less than 0.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train VW with parameters selected on the validation set, but first merge the training and validation sets. Evaluate the share of correct answers on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "!cat stackoverflow_train.vw stackoverflow_valid.vw > stackoverflow_train_big.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = stackoverflow_test_preds.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = stackoverflow_test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        7      354\n",
      "0.500000 0.000000            2            2.0        7        7      146\n",
      "0.250000 0.000000            4            4.0        5        5      516\n",
      "0.125000 0.000000            8            8.0        7        7      286\n",
      "0.062500 0.000000           16           16.0        6        6      716\n",
      "0.031250 0.000000           32           32.0        2        2      798\n",
      "0.046875 0.062500           64           64.0        5        5     2120\n",
      "0.046875 0.046875          128          128.0        2        2      262\n",
      "0.070312 0.093750          256          256.0        6        6      174\n",
      "0.054688 0.039062          512          512.0        7        7       68\n",
      "0.054688 0.054688         1024         1024.0        1        1      174\n",
      "0.057617 0.060547         2048         2048.0        2        2      484\n",
      "0.056396 0.055176         4096         4096.0        2        2      524\n",
      "0.059570 0.062744         8192         8192.0        7        7      542\n",
      "0.063660 0.067749        16384        16384.0        2        2      252\n",
      "0.063599 0.063538        32768        32768.0        1        1      240\n",
      "0.064407 0.065216        65536        65536.0        2        2      148\n",
      "0.064301 0.064194       131072       131072.0        2        2       30\n",
      "0.064327 0.064354       262144       262144.0        6        6      162\n",
      "0.064531 0.064735       524288       524288.0        5        5      644\n",
      "0.064657 0.064783      1048576      1048576.0        4        2      210\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.064465\n",
      "total feature number = 580609416\n"
     ]
    }
   ],
   "source": [
    "#!vw -d stackoverflow_train_big.vw --oaa 10 --ngram 2 --passes 1 -b 28 --random_seed 17 --readable_model readable.vw.model -f vw.model -c\n",
    "!vw -d stackoverflow_test.vw -t -i vw.model -p stackoverflow_test_preds.txt\n",
    "y_pred = pd.read_csv('stackoverflow_test_preds.txt', names=['pred'])\n",
    "acc = accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44538071301925\n"
     ]
    }
   ],
   "source": [
    "print((acc-0.9310815041236676)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** How large is the gain after training with 2x the data (training `stackoverflow_train.vw` + validation `stackoverflow_valid.vw`) versus the model trained solely on `stackoverflow_train.vw`?\n",
    " - 0.1%\n",
    " - ##### 0.4%\n",
    " - 0.8%\n",
    " - 1.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have only just scratched the surface with Vowpal Wabbit in this assignment. Here are some hints on what to do next:\n",
    " - multilabel classification (`multilabel_oaa` argument) – data format perfectly matches with this type of problem\n",
    " - Tuning VW parameters with hyperopt. VW developers say that the accuracy strongly depends on gradient descent (`initial_t` and `power_t`) parameters. Also, we can test different loss functions i.e. train logistic regression and linear SVM\n",
    " - Learn about factorization machines and its implementation in VW (the `lrq` argument)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

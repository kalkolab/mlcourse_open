{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Open Machine Learning Course\n",
    "<center>Author: Yuriy Isakov\n",
    "\n",
    "All the materials are distributed under [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. You may use these materials for any non-commercial purposes with obligatory referencing to the original source and author. Any commercial usage (complete or partial) is prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Assignment #4\n",
    "## <center>  User Identification with Logistic Regression\n",
    "\n",
    "Today we are going to practice working with sparse matrices, training Logistic Regression models, and composing and sorting features. We will reproduce two baselines for the [\"Catch Me If You Can: Intruder Detection through Webpage Session Tracking\"](https://www.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2)  Kaggle inclass competition. \n",
    "\n",
    "This assignment consists of nine tasks which you will need to perform. When you are finished, submit the [google-form](https://docs.google.com/forms/d/1dgk3zk-gjuJjazFZ3DTjMHvVhMbrzAuErufSpN6bxKA/edit) with your answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we don't like warnings\n",
    "# you can comment the following 2 lines if you'd like to\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import libraries \n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approaching the Problem\n",
    "We will be solving the intruder detection problem by analyzing his behavior on the Internet. It is a complicated and interesting problem that combines data analysis and behavioral psychology.\n",
    "\n",
    "For example, Yandex solves the mailbox intruder detection problem based on the user's behavior patterns based on the notion that the intruder's behaviour pattern might differ from the owner's: \n",
    "- the intruder might not delete emails right after they are read like the mailbox owner would\n",
    "- the intruder might mark emails or move the cursor differently\n",
    "- and more\n",
    "\n",
    "Therefore, the intruder could be detected and thrown out from the mailbox, forcing the user to authenticate via SMS-code.\n",
    "This pilot project is described in the Habrahabr article.\n",
    "\n",
    "Similar things are being developed in Google Analytics and the research community. You can find more on this topic by searching for \"Traversal Pattern Mining\" and \"Sequential Pattern Mining\".\n",
    "\n",
    "In this competition, we are going to solve a similar problem. Our algorithm needs to analyze the sequence of websites consequently visited by a particular person and predict whether this person is Alice or an intruder (someone else). We will measure [ROC AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). Stay tuned until the end of the course to find out who Alice is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Downloading and Transformation\n",
    "Register on [Kaggle](www.kaggle.com) if you have not done so before.\n",
    "Go to the competition [page](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) and download the data.\n",
    "\n",
    "First, read the training and test sets. Then, explore the data and perform a couple of simple exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21669</th>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54843</th>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2013-01-12 09:07:07</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 09:07:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77292</th>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:13</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:14</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>784.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114021</th>\n",
       "      <td>945</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146670</th>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1               time1  site2               time2  site3  \\\n",
       "session_id                                                                \n",
       "21669          56 2013-01-12 08:05:57   55.0 2013-01-12 08:05:57    NaN   \n",
       "54843          56 2013-01-12 08:37:23   55.0 2013-01-12 08:37:23   56.0   \n",
       "77292         946 2013-01-12 08:50:13  946.0 2013-01-12 08:50:14  951.0   \n",
       "114021        945 2013-01-12 08:50:17  948.0 2013-01-12 08:50:17  949.0   \n",
       "146670        947 2013-01-12 08:50:20  950.0 2013-01-12 08:50:20  948.0   \n",
       "\n",
       "                         time3  site4               time4  site5  \\\n",
       "session_id                                                         \n",
       "21669                      NaT    NaN                 NaT    NaN   \n",
       "54843      2013-01-12 09:07:07   55.0 2013-01-12 09:07:09    NaN   \n",
       "77292      2013-01-12 08:50:15  946.0 2013-01-12 08:50:15  946.0   \n",
       "114021     2013-01-12 08:50:18  948.0 2013-01-12 08:50:18  945.0   \n",
       "146670     2013-01-12 08:50:20  947.0 2013-01-12 08:50:21  950.0   \n",
       "\n",
       "                         time5  ...                 time6  site7  \\\n",
       "session_id                      ...                                \n",
       "21669                      NaT  ...                   NaT    NaN   \n",
       "54843                      NaT  ...                   NaT    NaN   \n",
       "77292      2013-01-12 08:50:16  ...   2013-01-12 08:50:16  948.0   \n",
       "114021     2013-01-12 08:50:18  ...   2013-01-12 08:50:18  947.0   \n",
       "146670     2013-01-12 08:50:21  ...   2013-01-12 08:50:21  946.0   \n",
       "\n",
       "                         time7  site8               time8  site9  \\\n",
       "session_id                                                         \n",
       "21669                      NaT    NaN                 NaT    NaN   \n",
       "54843                      NaT    NaN                 NaT    NaN   \n",
       "77292      2013-01-12 08:50:16  784.0 2013-01-12 08:50:16  949.0   \n",
       "114021     2013-01-12 08:50:19  945.0 2013-01-12 08:50:19  946.0   \n",
       "146670     2013-01-12 08:50:21  951.0 2013-01-12 08:50:22  946.0   \n",
       "\n",
       "                         time9 site10              time10 target  \n",
       "session_id                                                        \n",
       "21669                      NaT    NaN                 NaT      0  \n",
       "54843                      NaT    NaN                 NaT      0  \n",
       "77292      2013-01-12 08:50:17  946.0 2013-01-12 08:50:17      0  \n",
       "114021     2013-01-12 08:50:19  946.0 2013-01-12 08:50:20      0  \n",
       "146670     2013-01-12 08:50:22  947.0 2013-01-12 08:50:22      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the training and test data sets\n",
    "train_df = pd.read_csv('../../data/websites_train_sessions.csv',\n",
    "                       index_col='session_id')\n",
    "test_df = pd.read_csv('../../data/websites_test_sessions.csv',\n",
    "                      index_col='session_id')\n",
    "\n",
    "# Switch time1, ..., time10 columns to datetime type\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "train_df[times] = train_df[times].apply(pd.to_datetime)\n",
    "test_df[times] = test_df[times].apply(pd.to_datetime)\n",
    "\n",
    "# Sort the data by time\n",
    "train_df = train_df.sort_values(by='time1')\n",
    "\n",
    "# Look at the first rows of the training set\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset contains the following features:\n",
    "\n",
    "- **site1** – id of the first visited website in the session\n",
    "- **time1** – visiting time for the first website in the session\n",
    "- ...\n",
    "- **site10** – id of the tenth visited website in the session\n",
    "- **time10** – visiting time for the tenth website in the session\n",
    "- **target** – target variable, value 1 for Alice's sessions, and 0 otherwise\n",
    "    \n",
    "User sessions are chosen in such a way that they are no longer than half an hour and/or contain more than ten websites i.e. a session is considered ended if either a user has visited ten websites or a session has lasted for more than thirty minutes.\n",
    "\n",
    "There are some empty values in the table, which means that these sessions contain less than ten websites. Replace empty values with 0, and change the columns' types to integer. Load the website's dictionary and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Websites total: 48371\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25075</th>\n",
       "      <td>www.abmecatronique.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>groups.live.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42436</th>\n",
       "      <td>majeureliguefootball.wordpress.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30911</th>\n",
       "      <td>cdt46.media.tourinsoft.eu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8104</th>\n",
       "      <td>www.hdwallpapers.eu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     site\n",
       "25075              www.abmecatronique.com\n",
       "13997                     groups.live.com\n",
       "42436  majeureliguefootball.wordpress.com\n",
       "30911           cdt46.media.tourinsoft.eu\n",
       "8104                  www.hdwallpapers.eu"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change site1, ..., site10 columns type to integer and fill NA-values with zeros\n",
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "train_df[sites] = train_df[sites].fillna(0).astype('int')\n",
    "test_df[sites] = test_df[sites].fillna(0).astype('int')\n",
    "\n",
    "# Load websites dictionary\n",
    "with open(r\"../../data/site_dic.pkl\", \"rb\") as input_file:\n",
    "    site_dict = pickle.load(input_file)\n",
    "\n",
    "# Create dataframe for the dictionary\n",
    "sites_dict = pd.DataFrame(list(site_dict.keys()), index=list(site_dict.values()), columns=['site'])\n",
    "print(u'Websites total:', sites_dict.shape[0])\n",
    "sites_dict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 4.1. What are the dimensions of the training and test sets (in exactly this order)?\n",
    "\n",
    "- (82797, 20) and (253561, 20)\n",
    "- (82797, 20) and (253561, 21)\n",
    "- (253561, 21) and (82797, 20)\n",
    "- (253561, 20) and (82797, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253561, 21)\n",
      "(82797, 20)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Your code is her\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Brief Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training models, we have to perform Exploratory Data Analysis ([EDA](https://en.wikipedia.org/wiki/Exploratory_data_analysis)). Today, we are going to perform a shorter version, but we will use other techniques as we move forward. Let's check which websites in the training data set are the most visited. As you can see, they are Google services and a bioinformatics website (a website with 'zero'-index is our missed values, just ignore it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21     123776\n",
      "0      122730\n",
      "23      87619\n",
      "782     77055\n",
      "22      58258\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>www.google.fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>www.google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>annotathon.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>apis.google.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                site\n",
       "21     www.google.fr\n",
       "0                NaN\n",
       "23    www.google.com\n",
       "782   annotathon.org\n",
       "22   apis.google.com"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top websites in the training data set\n",
    "top_sites = pd.Series(train_df[sites].fillna(0).values.flatten()\n",
    "                     ).value_counts().sort_values(ascending=False).head(5)\n",
    "print(top_sites)\n",
    "sites_dict.loc[top_sites.index.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2. What kind of websites does Alice visit the most?\n",
    "\n",
    "- videohostings\n",
    "- social networks\n",
    "- torrent trackers\n",
    "- news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77    1382\n",
      "80    1354\n",
      "76    1307\n",
      "29     897\n",
      "21     857\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>i1.ytimg.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>s.youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>www.youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>www.facebook.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>www.google.fr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                site\n",
       "77      i1.ytimg.com\n",
       "80     s.youtube.com\n",
       "76   www.youtube.com\n",
       "29  www.facebook.com\n",
       "21     www.google.fr"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code is here\n",
    "alice_df = train_df[train_df['target']==1]\n",
    "alice_top = pd.Series(alice_df[sites].fillna(0).values.flatten()\n",
    "                     ).value_counts().sort_values(ascending=False).head(5)\n",
    "print(alice_top)\n",
    "sites_dict.loc[alice_top.index.tolist()]\n",
    "\n",
    "#video hostings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us look at the timestamps and try to characterize sessions by timeframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>seconds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21669</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54843</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>2013-01-12 09:07:09</td>\n",
       "      <td>1786.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77292</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-12 08:50:13</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114021</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146670</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            target                 min                 max  seconds\n",
       "session_id                                                         \n",
       "21669            0 2013-01-12 08:05:57 2013-01-12 08:05:57      0.0\n",
       "54843            0 2013-01-12 08:37:23 2013-01-12 09:07:09   1786.0\n",
       "77292            0 2013-01-12 08:50:13 2013-01-12 08:50:17      4.0\n",
       "114021           0 2013-01-12 08:50:17 2013-01-12 08:50:20      3.0\n",
       "146670           0 2013-01-12 08:50:20 2013-01-12 08:50:22      2.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a separate dataframe where we will work with timestamps\n",
    "time_df = pd.DataFrame(index=train_df.index)\n",
    "time_df['target'] = train_df['target']\n",
    "\n",
    "# Find sessions' starting and ending\n",
    "time_df['min'] = train_df[times].min(axis=1)\n",
    "time_df['max'] = train_df[times].max(axis=1)\n",
    "\n",
    "# Calculate sessions' duration in seconds\n",
    "time_df['seconds'] = (time_df['max'] - time_df['min']) / np.timedelta64(1, 's')\n",
    "\n",
    "time_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform the next task, generate descriptive statistics as you did for the first homework in the first week.\n",
    "\n",
    "##### 4.3. Select the correct statements (there can be none, one,.., all correct statements):\n",
    "\n",
    "- on average, Alice's session is shorter than that of other users\n",
    "- more than 1% of all sessions in the dataset belong to Alice\n",
    "- minimum and maximum durations of Alice's and other users' sessions are approximately the same\n",
    "- variation about the mean session duration for all users (including Alice) is approximately the same\n",
    "- less than a quarter of Alice's sessions are greater than or equal to 40 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    139.282372\n",
      "1     52.296474\n",
      "Name: seconds, dtype: float64\n",
      "0    0.990941\n",
      "1    0.009059\n",
      "Name: target, dtype: float64\n",
      "target\n",
      "0    1800.0\n",
      "1    1763.0\n",
      "Name: seconds, dtype: float64\n",
      "target\n",
      "0    0.0\n",
      "1    0.0\n",
      "Name: seconds, dtype: float64\n",
      "target\n",
      "0    296.653518\n",
      "1    153.309014\n",
      "Name: seconds, dtype: float64\n",
      "0.2411841532433609\n"
     ]
    }
   ],
   "source": [
    "# Your code is here\n",
    "print(time_df.groupby('target')['seconds'].mean()) # alice session is shorter\n",
    "print(time_df['target'].value_counts(normalize=True)) # alice < 1%\n",
    "print(time_df.groupby('target')['seconds'].max())  # the same\n",
    "print(time_df.groupby('target')['seconds'].min())# the same\n",
    "print(time_df.groupby('target')['seconds'].std())  #variation not the same\n",
    "alice_sessions = time_df[time_df['target']==1]\n",
    "print(alice_sessions[alice_sessions['seconds']>=40].shape[0] / alice_sessions.shape[0]) # less than a quarter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our first model, we need to prepare the data. First, exclude the target variable from the training set. Both training and test sets now have the same number of columns, and we can aggregate them into a single dataframe.  All transformations will therefore be performed simultaneously on both training and test data sets. \n",
    "\n",
    "On the one hand, this will lead to the fact that both data sets have one feature space (you don't have to worry that you forgot to transform a feature in some data sets). On the other hand, processing time will increase. \n",
    "For enormously large sets, it might be impossible to transform both data sets simultaneously, and you willhave to split your transformations into several stages across the train/test data set).\n",
    "For this dataset, we are going to perform all the transformations for the whole combined dataframe at once and will filter the appropriate part before training the model or making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our target variable\n",
    "y_train = train_df['target']\n",
    "\n",
    "# United dataframe of the initial data \n",
    "full_df = pd.concat([train_df.drop('target', axis=1), test_df])\n",
    "\n",
    "# Index to split the training and test data sets\n",
    "idx_split = train_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the basic model, we will use only the visited websites in the session (but will not take into account the timestamp features). The point behind this data selection is: *Alice has her favorite sites. The more often you see these sites in the session, the higher the probability that this is Alice and vice versa.*\n",
    "\n",
    "Now we'll prepare the data, taking only features `site1, site2, ... , site10` from the whole dataframe. Keep in mind that the missing values have been replaced with zero. Here is how the first rows of the dataframe should look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21669</th>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54843</th>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77292</th>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "      <td>951</td>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "      <td>945</td>\n",
       "      <td>948</td>\n",
       "      <td>784</td>\n",
       "      <td>949</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114021</th>\n",
       "      <td>945</td>\n",
       "      <td>948</td>\n",
       "      <td>949</td>\n",
       "      <td>948</td>\n",
       "      <td>945</td>\n",
       "      <td>946</td>\n",
       "      <td>947</td>\n",
       "      <td>945</td>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146670</th>\n",
       "      <td>947</td>\n",
       "      <td>950</td>\n",
       "      <td>948</td>\n",
       "      <td>947</td>\n",
       "      <td>950</td>\n",
       "      <td>952</td>\n",
       "      <td>946</td>\n",
       "      <td>951</td>\n",
       "      <td>946</td>\n",
       "      <td>947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\n",
       "session_id                                                                  \n",
       "21669          56     55      0      0      0      0      0      0      0   \n",
       "54843          56     55     56     55      0      0      0      0      0   \n",
       "77292         946    946    951    946    946    945    948    784    949   \n",
       "114021        945    948    949    948    945    946    947    945    946   \n",
       "146670        947    950    948    947    950    952    946    951    946   \n",
       "\n",
       "            site10  \n",
       "session_id          \n",
       "21669            0  \n",
       "54843            0  \n",
       "77292          946  \n",
       "114021         946  \n",
       "146670         947  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe with indices of visited websites in session\n",
    "full_sites = full_df[sites]\n",
    "full_sites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sessions are the sequences of website indices. This representation is inconvenient for linear methods. According to our hypothesis (Alice has favorite websites), we need to transform this dataframe so that each website has its corresponding feature (column) and that its value is equal to number of visits in the session. It can be accomplished with two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sequence of indices\n",
    "sites_flatten = full_sites.values.flatten()\n",
    "\n",
    "# and the matrix we are looking for\n",
    "full_sites_sparse = csr_matrix(([1] * sites_flatten.shape[0],\n",
    "                                sites_flatten,\n",
    "                                range(0, sites_flatten.shape[0]  + 10, 10)))[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you understand what just happened here, then feel free to skip the next passage. If not, then let us figure it out.\n",
    "\n",
    "### Lyrical Digression 1: Sparse Matrices\n",
    "\n",
    "Let us estimate how much memory it will require to store our data from the example above. Our united dataframe contains 336k samples with 48k integer features each. We can roughly calculate the required amount of memory as follows:\n",
    "\n",
    "$$336K * 48K * 8 bytes = 16M * 8 bytes = 128 GB,$$\n",
    "\n",
    "(that's an [exact](http://www.wolframalpha.com/input/?i=336358*48371*8+bytes) value). Obviously, ordinary mortals do not have such volumes. (Python may allow you to create such a matrix, but it will not be easy to do anything with it.) However, recall that most of the elements in our matrix are zeros. If we count non-zero elements, then it will be only about 1.8 million, slightly more than 10% of the elements in the matrix. Such a matrix, where most elements are zeros, is considered sparse. The ratio between the number of zero elements and the total number of elements is called the sparseness of the matrix.\n",
    "\n",
    "For working with such matrices, you can use the `scipy.sparse` library. Check [documentation](https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html) to understand what possible types of sparse matrices there are, how to work with them, and in which cases their usage is most effective. You can learn how they are arranged, for example, in Wikipedia [article](https://en.wikipedia.org/wiki/Sparse_matrix).\n",
    "Note that a sparse matrix stores only the non-zero elements, and you can get the allocated memory size by doing this (significant memory savings are obvious):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1866898 elements * 8 bytes = 14935184 bytes\n",
      "sparse_matrix_size = 7467592 bytes\n"
     ]
    }
   ],
   "source": [
    "# How much memory does a sparse matrix occupy?\n",
    "print('{0} elements * {1} bytes = {2} bytes'.format(full_sites_sparse.count_nonzero(), 8, \n",
    "                                                    full_sites_sparse.count_nonzero() * 8))\n",
    "# Or just like this:\n",
    "print('sparse_matrix_size = {0} bytes'.format(full_sites_sparse.data.nbytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore how the matrix with the websites was formed using a small example. Suppose we have the following table with user sessions:\n",
    "\n",
    "| id | site1 | site2 | site3 |\n",
    "|---|---|---|---|\n",
    "| 1 | 1 | 0 | 0 |\n",
    "| 2 | 1 | 3 | 1 |\n",
    "| 3 | 2 | 3 | 4 |\n",
    "\n",
    "There are 3 sessions with no more than 3 websites each. Users only visited four different sites in total (indexed as 1 to 4 in the table cells):\n",
    "\n",
    " 1. vk.com\n",
    " 2. habrahabr.ru \n",
    " 3. yandex.ru\n",
    " 4. ods.ai\n",
    "\n",
    "If the user has visited fewer than 3 websites during the session, the last few values will be zero. We want to convert the original dataframe in a way that each session has a corresponding row which shows the number of visits to each particular site i.e. we want to transform the previous table into the following form:\n",
    "\n",
    "| id | vk.com | habrahabr.ru | yandex.ru | ods.ai |\n",
    "|---|---|---|---|---|\n",
    "| 1 | 1 | 0 | 0 | 0 |\n",
    "| 2 | 2 | 0 | 1 | 0 |\n",
    "| 3 | 0 | 1 | 1 | 1 |\n",
    "\n",
    "\n",
    "To do this, use the constructor: `csr_matrix ((data, indices, indptr))` and create a frequency table (see examples, code, and comments in the links above to see how this works). Here we set all the parameters explicitly for greater clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[2, 1, 0, 0, 0],\n",
       "        [0, 2, 0, 1, 0],\n",
       "        [0, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data, create the list of ones, length of which equal to the number of elements in the initial dataframe (9)\n",
    "# By summing the number of ones in the cell, we get the frequency,\n",
    "# number of visits to a particular site per session\n",
    "data = [1] * 9\n",
    "\n",
    "# To do this, you need to correctly distribute the ones in cells\n",
    "# Indices - website ids, i.e. columns of a new matrix. We will sum ones up grouping them by sessions (ids)\n",
    "indices = [1, 0, 0, 1, 3, 1, 2, 3, 4]\n",
    "\n",
    "# Indices for the division into rows (sessions)\n",
    "# For example, line 0 is the elements between the indices [0; 3) - the rightmost value is not included\n",
    "# Line 1 is the elements between the indices [3; 6)\n",
    "# Line 2 is the elements between the indices [6; 9) \n",
    "indptr = [0, 3, 6, 9]\n",
    "\n",
    "# Aggregate these three variables into a tuple and compose a matrix\n",
    "# To display this matrix on the screen transform it into the usual \"dense\" matrix\n",
    "csr_matrix((data, indices, indptr)).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have noticed, there are not four columns in the resulting matrix (corresponding to number of different websites) but five. A zero column has been added, which indicates if the session was shorter (in our mini example we took sessions of three). This column is excessive and should be removed from the dataframe (do that yourself).\n",
    "\n",
    "##### 4.4. What is the sparseness of the matrix in our small example?\n",
    "\n",
    "- 42%\n",
    "- 47%\n",
    "- 50%\n",
    "- 53%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [2 0 1 0]\n",
      " [0 1 1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code is here\n",
    "m = csr_matrix((data, indices, indptr))[:, 1:]\n",
    "print(m.todense())\n",
    "1- m.count_nonzero()/m.toarray().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another benefit of using sparse matrices is that there are special implementations of both matrix operations and machine learning algorithms for them, which sometimes allows significantly faster operations due to the data structure. This applies to logistic regression as well. Now everything is ready to build our first model.\n",
    "\n",
    "### 3. Training the first model\n",
    "\n",
    "We have an algorithm and data for it. Let's build our first model using the [logistic regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) implementation from ` sklearn` with default parameters. We will use the first 90% of the data for training (sorted by time) and the remaining 10% for validation. Let's write a simple function that returns the quality of the model and then train our first classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_auc_lr_valid(X, y, C=1.0, seed=17, ratio = 0.9):\n",
    "    # Split the data into the training and validation sets\n",
    "    idx = int(round(X.shape[0] * ratio))\n",
    "    # Classifier training\n",
    "    lr = LogisticRegression(C=C, random_state=seed).fit(X[:idx, :], y[:idx])\n",
    "    # Prediction for validation set\n",
    "    y_pred = lr.predict_proba(X[idx:, :])[:, 1]\n",
    "    # Calculate the quality\n",
    "    score = roc_auc_score(y[idx:], y_pred)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.919524256796\n",
      "Wall time: 9.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Select the training set from the united dataframe (where we have the answers)\n",
    "X_train = full_sites_sparse[:idx_split, :]\n",
    "\n",
    "# Calculate metric on the validation set\n",
    "print(get_auc_lr_valid(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model achieved an accuracy of 0.91952 on the validation set. The will be the first baseline and starting point. To make a prediction on the test data set, ** we need to train the model again on the entire training dataset ** Up until now, our model used only part of the data for training; this will now increase its generalizing ability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for writing predictions to a file\n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model on the whole training data set\n",
    "# Use random_state=17 for repeatability\n",
    "# Parameter C=1 by default, but here we set it explicitly\n",
    "lr = LogisticRegression(C=1.0, random_state=17).fit(X_train, y_train)\n",
    "\n",
    "# Make a prediction for test data set\n",
    "X_test = full_sites_sparse[idx_split:,:]\n",
    "y_test = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Write it to the file which could be submitted\n",
    "write_to_submission_file(y_test, 'baseline_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you follow these steps and upload the answer to the competition [page](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2), you will get `ROC AUC = 0.90812` on the public leaderboard.\n",
    "\n",
    "### 4. Model Improvement: New Features Engineering\n",
    "\n",
    "Now we are going to try to improve the quality of our model by adding new features to the data. But first, answer the following question:\n",
    "\n",
    "##### 4.5. What years are present in the training and test datasets, respectively?\n",
    "\n",
    "- 13 and 14\n",
    "- 2012 and 2013\n",
    "- 2013 and 2014\n",
    "- 2014 and 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014.0    1706973\n",
       "2013.0     705907\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.array([x.year for x in ])\n",
    "pd.Series(pd.to_datetime(train_df[times].values.flatten()).year).value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Your code is here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feature that will be a number in YYYYMM format for the date when the session was held, for example 201407 -- year 2014 and 7th month. We will take into account the monthly [linear trend](http://people.duke.edu/~rnau/411trend.htm) for the entire period of the data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataframe for new features\n",
    "full_new_feat = pd.DataFrame(index=full_df.index)\n",
    "\n",
    "# Add start_month feature\n",
    "full_new_feat['start_month'] = full_df['time1'].apply(lambda ts: 100 * ts.year + ts.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.6. Plot the graph of the number of Alice sessions versus the new feature, start_month. Choose the correct statement:\n",
    "\n",
    "- Alice wasn't online at all for the entire period\n",
    "- From the beginning of 2013 to mid-2014, the number of Alice's sessions per month decreased\n",
    "- The number of Alice's sessions per month is generally constant for the entire period\n",
    "- From the beginning of 2013 to mid-2014, the number of Alice's sessions per month increased\n",
    "\n",
    "*Hint: the graph will be more explicit if you treat `start_month` as a categorical ordinal variable *."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start_month\n",
       "201302     61\n",
       "201304     38\n",
       "201309    377\n",
       "201311    446\n",
       "201312    134\n",
       "201401    129\n",
       "201402    410\n",
       "201403    400\n",
       "201404    302\n",
       "201405    229\n",
       "201406     54\n",
       "201407     83\n",
       "201408     69\n",
       "201409     37\n",
       "201410     80\n",
       "201411     84\n",
       "201412    121\n",
       "dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code is here\n",
    "full_new_feat[y_train==1].groupby('start_month').size() #flase false flase true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an illustration and some intuition about the usefulness of this new feature. Add it to the training sample, and check the quality of the new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.750835486018\n"
     ]
    }
   ],
   "source": [
    "# Add the new feature to the sparse matrix\n",
    "tmp = full_new_feat[['start_month']].as_matrix()\n",
    "X_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], tmp[:idx_split,:]]))\n",
    "\n",
    "# Compute the metric on the validation set\n",
    "print(get_auc_lr_valid(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of the model has decreased significantly. We added a feature that definitely seemed useful to us, but it only made the model worse. Why did that happen?\n",
    "\n",
    "### Lyrical Digression 2: is it necessary to scale the features?\n",
    "\n",
    "Here we will use intuitive reasoning (you can easily find a rigorous mathematical justification for linear models on the internet). Consider the features more closely: those that correspond to the number of visits to a particular web-site per session vary from 0 to 10. The feature `start_month` has a completely different range, 201301 to 201412; this means the contribution from this variable is significantly greater than the others. It would seem that problem can be avoided if we put less weight on a linear combination of attributes in this case, but we are using logistic regression with regularization (by default, this parameter is `C = 1`), which penalizes the model the stronger its weights are. Therefore, for linear methods with regularization, it is recommended to convert features to the same scale (you can read more about regularization [here](https://habrahabr.ru/company/ods/blog/322076/)).\n",
    "\n",
    "One way to do this is with standardization: for each observation you need to subtract the average value of the feature and divide the difference by the standard deviation:\n",
    "\n",
    "$$ x^{*}_{i} = \\dfrac{x_{i} - \\mu_x}{\\sigma_x}$$\n",
    "\n",
    "Here are some practical tips:\n",
    "- It is recommended to scale features if they have essentially different ranges or different units of measurement (for example, the country's population is in millions, and the country's GNP is in trillions)\n",
    "- Scale features if you do not have a reason or opinion to give a greater weight to any of them\n",
    "- Scaling can be excessive if the ranges of some of your features differ from each other but have the same units (for example, the proportion of middle-aged people and the proportion of people over 80)\n",
    "- If you want to get an interpretable model, then build a model without regularization and scaling (most likely, its quality will be worse)\n",
    "- Binary features (which take only values of 0 or 1) are usually left without conversion, but...\n",
    "- If the quality of the model is crucial, try different options and select the one where the performance is better\n",
    "\n",
    "Keeping this in mind, let's return to `start_month` and rescale the feature to train the model again. This time, we see that the quality has increased:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.919683066316\n"
     ]
    }
   ],
   "source": [
    "# Add the new standardized feature to the sparse matrix\n",
    "tmp = StandardScaler().fit_transform(full_new_feat[['start_month']])\n",
    "X_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], tmp[:idx_split,:]]))\n",
    "\n",
    "# Compute metric on the validation set\n",
    "print(get_auc_lr_valid(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.7. Add to the training set a new feature \"n_unique_sites\" – the number of the unique web-sites in a session. Calculate how the quality on the validation set has changed\n",
    "\n",
    "- It has decreased. It is better not to add a new feature.\n",
    "- It has not changed\n",
    "- It has decreased. The new feature should be scaled.\n",
    "- I am confused, and I do not know if it's necessary to scale a new feature.\n",
    "\n",
    "*Tips: use the nunique() function from `pandas`. Do not forget to include the start_month in the set. Will you scale a new feature? Why?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.916343764039\n"
     ]
    }
   ],
   "source": [
    "# Your code is here\n",
    "full_new_feat['n_unique_sites'] = full_df[sites].apply(lambda row: row.nunique(), axis = 1, raw=False) \n",
    "\n",
    "st_month = StandardScaler().fit_transform(full_new_feat[['start_month']])\n",
    "\n",
    "n_un_sites = full_new_feat[['n_unique_sites']].as_matrix()\n",
    "\n",
    "X_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], st_month[:idx_split,:], n_un_sites[:idx_split,:]]))\n",
    "\n",
    "# Compute metric on the validation set\n",
    "print(get_auc_lr_valid(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the new feature has slightly decreased the quality, so we will not use it. Nevertheless, do not rush to throw features out because they haven't performed well. They can be useful in a combination with other features (for example, when a new feature is a ratio or a product of two others).\n",
    "\n",
    "#####  4.8. Add two new features: start_hour and morning. Calculate the metric. Which of these features gives an improvement?\n",
    "\n",
    "The `start_hour` feature is the hour at which the session started (from 0 to 23), and the binary feature `morning` is equal to 1 if the session started in the morning and 0 if the session started later (we assume that morning means `start_hour` is equal to 11 or less).\n",
    "\n",
    "Will you scale the new features? Make your assumptions and test them in practice.\n",
    "\n",
    "- None of the features gave an improvement :(\n",
    "- `start_hour` feature gave an improvement, and `morning` did not\n",
    "- `morning` feature gave an improvement, and `start_hour` did not\n",
    "- Both features gave an improvement\n",
    "\n",
    "*Tip: find suitable functions for working with time series data in [documentation](http://pandas.pydata.org/pandas-docs/stable/api.html). Do not forget to include the `start_month` feature.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_month</th>\n",
       "      <th>n_unique_sites</th>\n",
       "      <th>start_hour</th>\n",
       "      <th>morning</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21669</th>\n",
       "      <td>201301</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54843</th>\n",
       "      <td>201301</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77292</th>\n",
       "      <td>201301</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114021</th>\n",
       "      <td>201301</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146670</th>\n",
       "      <td>201301</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            start_month  n_unique_sites  start_hour  morning\n",
       "session_id                                                  \n",
       "21669            201301               3           8        1\n",
       "54843            201301               3           8        1\n",
       "77292            201301               6           8        1\n",
       "114021           201301               5           8        1\n",
       "146670           201301               6           8        1"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code is here\n",
    "full_new_feat['start_hour'] = full_df['time1'].apply(lambda ts: ts.hour)\n",
    "full_new_feat['morning'] = (full_new_feat['start_hour'] <= 11).astype(int)\n",
    "full_new_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.957927478395\n"
     ]
    }
   ],
   "source": [
    "st_month = StandardScaler().fit_transform(full_new_feat[['start_month']])\n",
    "\n",
    "#st_hour = full_new_feat[['start_hour']].as_matrix()\n",
    "st_hour = StandardScaler().fit_transform(full_new_feat[['start_hour']])\n",
    "\n",
    "X_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], st_month[:idx_split,:], st_hour[:idx_split,:]]))\n",
    "print(get_auc_lr_valid(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95911749308\n"
     ]
    }
   ],
   "source": [
    "st_month = StandardScaler().fit_transform(full_new_feat[['start_month']])\n",
    "\n",
    "morning = full_new_feat[['morning']].as_matrix()\n",
    "#morning = StandardScaler().fit_transform(full_new_feat[['morning']])\n",
    "\n",
    "X_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], st_month[:idx_split,:], st_hour[:idx_split,:], morning[:idx_split,:]]))\n",
    "print(get_auc_lr_valid(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Regularization and Parameter Tuning\n",
    "\n",
    "We have introduced features that improve the quality of our model in comparison with the first baseline. Can we do even better? After we have changed the training and test sets, it almost always makes sense to search for the optimal hyperparameters - the parameters of the model that do not change during training.\n",
    "\n",
    "For example, in week 3, you learned that, in decision trees, the depth of the tree is a hyperparameter, but the feature by which splitting occurs and its threshold is not. \n",
    "\n",
    "In the logistic regression taht we use, the weights of each feature are changing, and we find their optimal values during training; meanwhile, the regularization parameter remains constant. This is the hyperparameter that we are going to optimize now.\n",
    "\n",
    "Calculate the quality on a validation set with a regularization parameter, which is equal to 1 by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.959152062833\n"
     ]
    }
   ],
   "source": [
    "# Compose the training set\n",
    "tmp_scaled = StandardScaler().fit_transform(full_new_feat[['start_month', 'start_hour', \n",
    "                                                           'morning']])\n",
    "X_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], \n",
    "                             tmp_scaled[:idx_split,:]]))\n",
    "\n",
    "# Capture the quality with default parameters\n",
    "score_C_1 = get_auc_lr_valid(X_train, y_train)\n",
    "print(score_C_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to beat this result by optimizing the regularization parameter. We will take a list of possible values of $C$ and calculate the quality metric on the validation set for each of $C$-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:54<00:00, 21.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# List of possible C-values\n",
    "Cs = np.logspace(-3, 1, 10)\n",
    "scores = []\n",
    "    \n",
    "#for C in Cs:\n",
    "#    scores.append(get_auc_lr_valid(X_train, y_train, C=C))\n",
    "\n",
    "\n",
    "\n",
    "# Install and import this module to track the number of iterations performed \n",
    "\n",
    "from tqdm import tqdm\n",
    "for C in tqdm(Cs):\n",
    "    scores.append(get_auc_lr_valid(X_train, y_train, C=C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the graph of the quality metric (AUC-ROC) versus the value of the regularization parameter. The value of quality metric corresponding to the default value of C=1 is represented by a horizontal dotted line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEaCAYAAAACBmAUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmZWEScMMzOriBOGmZlV5P8HUblJP/nX7M8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20201249d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Cs, scores, 'ro-')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('AUC-ROC')\n",
    "plt.title('Regularization Parameter Tuning')\n",
    "# horizontal line -- model quality with default C value\n",
    "plt.axhline(y=score_C_1, linewidth=.5, color = 'b', linestyle='dashed') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.9. What is the value of parameter C that corresponds to the highest model quality?\n",
    "\n",
    "- 0.17\n",
    "- 0.46\n",
    "- 1.29\n",
    "- 3.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16681005372\n"
     ]
    }
   ],
   "source": [
    "# Your code is here\n",
    "C = Cs[np.argmax(scores)]\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last task in this assignment: train the model using the optimal regularization parameter you found (do not round up to two digits like in the last question). If you do everything correctly and submit your solution, you should see 0.92784 on the public leaderboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the training and test data\n",
    "tmp_scaled = StandardScaler().fit_transform(full_new_feat[['start_month', 'start_hour', 'morning']])\n",
    "X_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], \n",
    "                             tmp_scaled[:idx_split,:]]))\n",
    "X_test = csr_matrix(hstack([full_sites_sparse[idx_split:,:], \n",
    "                            tmp_scaled[idx_split:,:]]))\n",
    "\n",
    "# Train the model on the whole training data set using optimal regularization parameter\n",
    "lr = LogisticRegression(C=C, random_state=17).fit(X_train, y_train)\n",
    "\n",
    "# Make a prediction for the test set\n",
    "y_test = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Write it to the submission file\n",
    "write_to_submission_file(y_test, 'baseline_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this assignment, you have learned how to use sparse matrices, train logistic regression models, create new features and selected the best ones, learned why you need to scale features, and how to select hyperparameters. (That's a lot!)\n",
    "\n",
    "Here are a few tips for finding new features: think about what you can come up with using existing features, try multiplying or dividing two of them, justify or decline your hypotheses with graphs, extract useful information from time series data (time1 ... time10), do not hesitate to convert an existing feature (for example, take a logarithm), etc. We encourage you to try new ideas and models throughout the course and participate in the competitions - it's fun!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
